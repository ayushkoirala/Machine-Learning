{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Ayush Koirala\"\n",
    "ID = \"st122802\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06: Generative classifiers: Naive Bayes\n",
    "\n",
    "As discussed in class, a Naive Bayes classifier works as follows:\n",
    "$$\\begin{eqnarray}\n",
    "p(y \\mid \\mathbf{x} ; \\theta) & = & \\frac{p(\\mathbf{x} \\mid y ; \\theta) p(y ; \\theta)}{p(\\mathbf{x} ; \\theta)} \\\\\n",
    "& \\propto & p(\\mathbf{x} \\mid y ; \\theta) p(y ; \\theta) \\\\\n",
    "& \\approx & p(y ; \\theta) \\prod_j p(x_j \\mid y ; \\theta)\n",
    "\\end{eqnarray}$$\n",
    "We will use Naive Bayes to perform diabetes diagnosis and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Diabetes classification\n",
    "\n",
    "In this example we predict wheter a patient with specific diagnostic measurements has diabetes or not. As the features are\n",
    "continuous, we will model the conditional probabilities\n",
    "$p(x_j \\mid y ; \\theta)$ as univariate Gaussians with mean $\\mu_{j,y}$ and standard deviation $\\sigma_{j,y}$.\n",
    "\n",
    "The data are originally from the U.S. National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) and are available\n",
    "from [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "\n",
    "First we have some functions to read the dataset, split it into train and test, and partition it according to target class ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "def loadCsv(filename):\n",
    "    data_raw = pd.read_csv(filename)\n",
    "    headers = data_raw.columns\n",
    "    dataset = data_raw.values\n",
    "    return dataset, headers\n",
    "\n",
    "# Split dataset into test and train with given ratio\n",
    "def splitDataset(test_size,*arrays,**kwargs):\n",
    "    return train_test_split(*arrays,test_size=test_size,**kwargs)\n",
    "\n",
    "# Separate training data according to target class\n",
    "# Return key value pairs array in which keys are possible target variable values\n",
    "# and values are the data records.\n",
    "\n",
    "def data_split_byClass(dataset):\n",
    "    Xy = {}\n",
    "    for i in range(len(dataset)):\n",
    "        datapair = dataset[i]\n",
    "        # datapair[-1] (the last column) is the target class for this record.\n",
    "        # Check if we already have this value as a key in the return array\n",
    "        if (datapair[-1] not in Xy):\n",
    "            # Add class as key\n",
    "            Xy[datapair[-1]] = []\n",
    "        # Append this record to array of records for this class key\n",
    "        Xy[datapair[-1]].append(datapair)\n",
    "    return Xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Next we have some functions used for training the model. Parameters include mean and standard deviation, used\n",
    "to partition numerical variables into categorical variables, as well as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of a Gaussian are its mean and standard deviation\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "# Calculate Gaussian parameters mu and sigma for each attribute over a dataset\n",
    "\n",
    "def get_gaussian_parameters(X,y):\n",
    "    parameters = {}\n",
    "    unique_y = np.unique(y)\n",
    "    for uy in unique_y:\n",
    "        mean = np.mean(X[y==uy],axis=0)\n",
    "        std = np.std(X[y==uy],axis=0)\n",
    "        py = y[y==uy].size/y.size\n",
    "        parameters[uy] = {'prior':py,'mean':mean,'std':std}\n",
    "    return parameters, unique_y\n",
    "\n",
    "def calculateProbability(x, mu, sigma):\n",
    "    sigma = np.diag(sigma**2)\n",
    "    x = x.reshape(-1,1)\n",
    "    mu = mu.reshape(-1,1)\n",
    "    exponent = np.exp(-1/2*(x-mu).T@np.linalg.inv(sigma)@(x-mu))\n",
    "    return ((1/(np.sqrt(((2*np.pi)**x.size)*np.linalg.det(sigma))))*exponent)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing\n",
    "\n",
    "Next some functions for testing the model on a test set and computing its accuracy. Note that we assume\n",
    "$$ p(y \\mid \\mathbf{x} ; \\theta) \\propto p(\\mathbf{x} \\mid y ; \\theta), $$\n",
    "which means we assume that the priors $p(y)$ are equal for each possible value of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class conditional probabilities for given input data vector\n",
    "\n",
    "def predict_one(x,parameters,unique_y,prior = True):\n",
    "    probabilities = []\n",
    "    for key in parameters.keys():\n",
    "        probabilities.append(calculateProbability(x,parameters[key]['mean'],parameters[key]['std'])*(parameters[key]['prior']**(float(prior))))\n",
    "    probabilities = np.array(probabilities)\n",
    "    return unique_y[np.argmax(probabilities)]\n",
    "\n",
    "def getPredictions(X, parameters, unique_y,prior=True):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        predictions.append(predict_one(X[i],parameters,unique_y,prior))\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Get accuracy for test set\n",
    "\n",
    "def getAccuracy(y, y_pred):\n",
    "    correct = len(y[y==y_pred])\n",
    "    return correct/y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "Here we load the diabetes dataset, split it into training and test data, train a Gaussian NB model, and test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 768 Train = 460 Test = 308\n",
      "Accuracy with Prior = 0.7564935064935064\n",
      "Accuracy without Prior = 0.737012987012987\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "filename = 'diabetes.csv'\n",
    "dataset, headers = loadCsv(filename)\n",
    "#print(headers)\n",
    "#print(np.array(dataset)[0:5,:])\n",
    "\n",
    "# Split into training and test\n",
    "\n",
    "X_train,X_test,y_train,y_test = splitDataset(0.4,dataset[:,:-1],dataset[:,-1])\n",
    "print(\"Total =\",len(dataset),\"Train =\", len(X_train),\"Test =\",len(X_test))\n",
    "\n",
    "# Train model\n",
    "\n",
    "parameters, unique_y = get_gaussian_parameters(X_train,y_train)\n",
    "prediction = getPredictions(X_test,parameters,unique_y)\n",
    "print(\"Accuracy with Prior =\",getAccuracy(y_test,prediction))\n",
    "\n",
    "# Test model\n",
    "\n",
    "prediction = getPredictions(X_test,parameters,unique_y,prior = False)\n",
    "print(\"Accuracy without Prior =\",getAccuracy(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0bcbc8407fdfb3f7d96cc07ad4d3124",
     "grade": false,
     "grade_id": "cell-9a740adb2ea13611",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###  Exercise In lab / take home work (20 points)\n",
    "\n",
    "Find out the proportion of the records in your dataset are positive vs. negative.  Can we conclude that $p(y=1) = p(y=0)$? If not, add\n",
    "the priors $p(y=1)$ and $p(y=0)$ to your NB model. Does it improve the result?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cf1a770bb1ee9faa6961f202769b10e",
     "grade": true,
     "grade_id": "cell-c1a5f74a2d330b0c",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prior, p(y=0.0): 0.6695652173913044\n",
      "The prior, p(y=1.0): 0.33043478260869563\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "for key,value in parameters.items():\n",
    "    print(f\"The prior, p(y={key}): {parameters[key]['prior']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f416d2fceb4d17b1865da03585d6f44f",
     "grade": false,
     "grade_id": "cell-a32adf75650dfd55",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Explain that you can conclude that $p(y=1) = p(y=0)$? If not, add\n",
    "the priors $p(y=1)$ and $p(y=0)$ to your NB model. Does it improve the result? (double click to explain)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above code shows that p(y=0) = 0.669 and p(y=1) = 0.33304 are not equal. When we compare the accuracy of our model with and without prior consideration in calculating probability, we can see that the accuracy with prior is greater than the accuracy without prior. This means that taking the prior into account improves the outcome.\n",
    "\n",
    "If p(y=0) and p(y=1) are equal, including priors in the prediction has no effect on accuracy because the probability p(y|x;theta) for both classes was multiplied by the same number. It's the same as scaling both classes' probality by the same constants. However, if the priors are unequal, or if the training dataset is not balanced (equal number of training samples for each class), the priors become biased more towards the class that occurs most frequently in the training samples. So, if we do not consider such priors to predict, it is equivalent to ignoring the probability of the test sample of such class occurring in the test dataset, which we assume to be the case.\n",
    "\n",
    "But if the probalility of occurence of the test sample of a class in the test dataset completely differed from that of training datasets then using the prior may result in poor performance. In such case you may not want to remove such prior from the model, meaning not using the prior to classify the test sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Text classification\n",
    "\n",
    "This example has been adapted from a post by Jaya Aiyappan, available at\n",
    "[Analytics Vidhya](https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b#:~:text=The%20Naive%20Bayes%20classifier%20is,time%20and%20less%20training%20data).\n",
    "\n",
    "We will generate a small dataset of sentences that are classified as either \"statements\" or \"questions.\"\n",
    "\n",
    "We will assume that occurance and placement of words within a sentence is independent of each other\n",
    "(i.e., the features are conditionally independent given $y$). So the sentence \"this is my book\" is the same as \"is this my book.\"\n",
    "We will treat words as case insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             sentence      class\n",
      "0               This is my novel book  statement\n",
      "1  this book has more than one author  statement\n",
      "2                     is this my book   question\n",
      "3                     They are novels  statement\n",
      "4             have you read this book   question\n",
      "5            who is the novels author   question\n",
      "6             what are the characters   question\n",
      "7       This is how I bought the book  statement\n",
      "8         I like fictional characters  statement\n",
      "9          what is your favorite book   question\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "                        sentence      class\n",
      "0               this is the book  statement\n",
      "1  who are the novels characters   question\n",
      "2             is this the author   question\n",
      "3                  I like apples       None\n"
     ]
    }
   ],
   "source": [
    "# Generate text data for two classes, \"statement\" and \"question\"\n",
    "\n",
    "text_train = [['This is my novel book', 'statement'],\n",
    "              ['this book has more than one author', 'statement'],\n",
    "              ['is this my book', 'question'],\n",
    "              ['They are novels', 'statement'],\n",
    "              ['have you read this book', 'question'],\n",
    "              ['who is the novels author', 'question'],\n",
    "              ['what are the characters', 'question'],\n",
    "              ['This is how I bought the book', 'statement'],\n",
    "              ['I like fictional characters', 'statement'],\n",
    "              ['what is your favorite book', 'question']]\n",
    "\n",
    "text_test = [['this is the book', 'statement'], \n",
    "             ['who are the novels characters', 'question'], \n",
    "             ['is this the author', 'question'],\n",
    "            ['I like apples']]\n",
    "\n",
    "# Load training and test data into pandas data frames\n",
    "\n",
    "training_data = pd.DataFrame(text_train, columns= ['sentence', 'class'])\n",
    "print(training_data)\n",
    "print('\\n------------------------------------------\\n')\n",
    "testing_data = pd.DataFrame(text_test, columns= ['sentence', 'class'])\n",
    "print(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition training data by class\n",
    "\n",
    "stmt_docs = [train['sentence'] for index,train in training_data.iterrows() if train['class'] == 'statement']\n",
    "question_docs = [train['sentence'] for index,train in training_data.iterrows() if train['class'] == 'question']\n",
    "all_docs = [train['sentence'] for index,train in training_data.iterrows()]\n",
    "\n",
    "# Get word frequencies for each sentence and class\n",
    "\n",
    "def get_words(text):\n",
    "    # Initialize word list\n",
    "    words = [];\n",
    "    # Loop through each sentence in input array\n",
    "    for text_row in text:       \n",
    "        # Check the number of words. Assume each word is separated by a blank space\n",
    "        # so that the number of words is the number of blank spaces + 1\n",
    "        number_of_spaces = text_row.count(' ')\n",
    "        # loop through the sentence and get words between blank spaces.\n",
    "        for i in range(number_of_spaces):\n",
    "            # Check for for last word\n",
    "            words.append([text_row[:text_row.index(' ')].lower()])\n",
    "            text_row = text_row[text_row.index(' ')+1:]  \n",
    "            i = i + 1        \n",
    "        words.append([text_row])\n",
    "    return np.unique(words)\n",
    "\n",
    "# Get frequency of each word in each document\n",
    "\n",
    "def get_doc_word_frequency(words, text):  \n",
    "    word_freq_table = np.zeros((len(text),len(words)), dtype=int)\n",
    "    i = 0\n",
    "    for text_row in text:\n",
    "        # Insert extra space between each pair of words to prevent\n",
    "        # partial match of words\n",
    "        text_row_temp = ''\n",
    "        for idx, val in enumerate(text_row):\n",
    "            if val == ' ':\n",
    "                 text_row_temp = text_row_temp + '  '\n",
    "            else:\n",
    "                  text_row_temp = text_row_temp + val.lower()\n",
    "        text_row = ' ' + text_row_temp + ' '\n",
    "        j = 0\n",
    "        for word in words: \n",
    "            word = ' ' + word + ' '\n",
    "            freq = text_row.count(word)\n",
    "            word_freq_table[i,j] = freq\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "    \n",
    "    return word_freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  author  book  bought  characters  fictional  has  how  i  is  like  \\\n",
      "0    0       0     1       0           0          0    0    0  0   1     0   \n",
      "1    0       1     1       0           0          0    1    0  0   0     0   \n",
      "2    1       0     0       0           0          0    0    0  0   0     0   \n",
      "3    0       0     1       1           0          0    0    1  1   1     0   \n",
      "4    0       0     0       0           1          1    0    0  1   0     1   \n",
      "\n",
      "   more  my  novel  novels  one  than  the  they  this  \n",
      "0     0   1      1       0    0     0    0     0     1  \n",
      "1     1   0      0       0    1     1    0     0     1  \n",
      "2     0   0      0       1    0     0    0     1     0  \n",
      "3     0   0      0       0    0     0    1     0     1  \n",
      "4     0   0      0       0    0     0    0     0     0  \n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies for statement documents\n",
    "\n",
    "word_list_s = get_words(stmt_docs)\n",
    "word_freq_table_s = get_doc_word_frequency(word_list_s, stmt_docs)\n",
    "tdm_s = pd.DataFrame(word_freq_table_s, columns=word_list_s)\n",
    "print(tdm_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are': 1, 'author': 1, 'book': 3, 'bought': 1, 'characters': 1, 'fictional': 1, 'has': 1, 'how': 1, 'i': 2, 'is': 2, 'like': 1, 'more': 1, 'my': 1, 'novel': 1, 'novels': 1, 'one': 1, 'than': 1, 'the': 1, 'they': 1, 'this': 3}\n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies over all statement documents\n",
    "\n",
    "freq_list_s = word_freq_table_s.sum(axis=0) \n",
    "freq_s = dict(zip(word_list_s,freq_list_s))\n",
    "print(freq_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  author  book  characters  favorite  have  is  my  novels  read  the  \\\n",
      "0    0       0     1           0         0     0   1   1       0     0    0   \n",
      "1    0       0     1           0         0     1   0   0       0     1    0   \n",
      "2    0       1     0           0         0     0   1   0       1     0    1   \n",
      "3    1       0     0           1         0     0   0   0       0     0    1   \n",
      "4    0       0     1           0         1     0   1   0       0     0    0   \n",
      "\n",
      "   this  what  who  you  your  \n",
      "0     1     0    0    0     0  \n",
      "1     1     0    0    1     0  \n",
      "2     0     0    1    0     0  \n",
      "3     0     1    0    0     0  \n",
      "4     0     1    0    0     1  \n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies for question documents\n",
    "\n",
    "word_list_q = get_words(question_docs)\n",
    "word_freq_table_q = get_doc_word_frequency(word_list_q, question_docs)\n",
    "tdm_q = pd.DataFrame(word_freq_table_q, columns=word_list_q)\n",
    "print(tdm_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are': 1, 'author': 1, 'book': 3, 'characters': 1, 'favorite': 1, 'have': 1, 'is': 3, 'my': 1, 'novels': 1, 'read': 1, 'the': 2, 'this': 2, 'what': 2, 'who': 1, 'you': 1, 'your': 1}\n",
      "[1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 3]\n",
      "[1 1 3 1 1 1 3 1 1 1 2 2 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies over all question documents\n",
    "\n",
    "freq_list_q = word_freq_table_q.sum(axis=0) \n",
    "freq_q = dict(zip(word_list_q,freq_list_q))\n",
    "print(freq_q)\n",
    "print(freq_list_s)\n",
    "print(freq_list_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words for \"statement\" class \n",
      "\n",
      "{'are': 0.043478260869565216, 'author': 0.043478260869565216, 'book': 0.08695652173913043, 'bought': 0.043478260869565216, 'characters': 0.043478260869565216, 'fictional': 0.043478260869565216, 'has': 0.043478260869565216, 'how': 0.043478260869565216, 'i': 0.06521739130434782, 'is': 0.06521739130434782, 'like': 0.043478260869565216, 'more': 0.043478260869565216, 'my': 0.043478260869565216, 'novel': 0.043478260869565216, 'novels': 0.043478260869565216, 'one': 0.043478260869565216, 'than': 0.043478260869565216, 'the': 0.043478260869565216, 'they': 0.043478260869565216, 'this': 0.08695652173913043}\n",
      "------------------------------------------- \n",
      "\n",
      "Probability of words for \"question\" class \n",
      "\n",
      "{'are': 0.05128205128205128, 'author': 0.05128205128205128, 'book': 0.10256410256410256, 'characters': 0.05128205128205128, 'favorite': 0.05128205128205128, 'have': 0.05128205128205128, 'is': 0.10256410256410256, 'my': 0.05128205128205128, 'novels': 0.05128205128205128, 'read': 0.05128205128205128, 'the': 0.07692307692307693, 'this': 0.07692307692307693, 'what': 0.07692307692307693, 'who': 0.05128205128205128, 'you': 0.05128205128205128, 'your': 0.05128205128205128}\n"
     ]
    }
   ],
   "source": [
    "# Get word probabilities for statement class\n",
    "a = 1\n",
    "prob_s = []\n",
    "for count in freq_list_s:\n",
    "    #print(word, count)\n",
    "    prob_s.append((count+a)/(sum(freq_list_s)+len(freq_list_s)*a))\n",
    "prob_s.append(a/(sum(freq_list_s)+len(freq_list_s)*a))\n",
    "    \n",
    "# Get word probabilities for question class\n",
    "\n",
    "prob_q = []\n",
    "for count in freq_list_q:\n",
    "    prob_q.append((count+a)/(sum(freq_list_q)+len(freq_list_q)*a))\n",
    "prob_q.append(a/(sum(freq_list_q)+len(freq_list_q)*a))   \n",
    "    \n",
    "    \n",
    "print('Probability of words for \"statement\" class \\n')\n",
    "print(dict(zip(word_list_s, prob_s)))\n",
    "print('------------------------------------------- \\n')\n",
    "print('Probability of words for \"question\" class \\n')\n",
    "print(dict(zip(word_list_q, prob_q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prior for one class\n",
    "\n",
    "def prior(className):    \n",
    "    denominator = len(stmt_docs) + len(question_docs)\n",
    "    \n",
    "    if className == 'statement':\n",
    "        numerator =  len(stmt_docs)\n",
    "    else:\n",
    "        numerator =  len(question_docs)\n",
    "        \n",
    "    return np.divide(numerator,denominator)\n",
    "    \n",
    "# Calculate class conditional probability for a sentence\n",
    "    \n",
    "def classCondProb(sentence, className):\n",
    "    words = get_words(sentence)\n",
    "    prob = 1\n",
    "    for word in words:\n",
    "        if className == 'statement':\n",
    "            idx = np.where(word_list_s == word)\n",
    "            prob = prob * prob_s[np.array(idx)[0,0]]\n",
    "        else:\n",
    "            idx = np.where(word_list_q == word)\n",
    "            prob = prob * prob_q[np.array(idx)[0,0]]   \n",
    "    \n",
    "    return prob\n",
    "\n",
    "# Predict class of a sentence\n",
    "\n",
    "def predict(sentence):\n",
    "    prob_statement = classCondProb(sentence, 'statement') * prior('statement')\n",
    "    prob_question = classCondProb(sentence, 'question') * prior('question')\n",
    "    if  prob_statement > prob_question:\n",
    "        return 'statement'\n",
    "    else:\n",
    "        return 'question'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06f31d4f4657c9d7a61ca287b3a51c86",
     "grade": false,
     "grade_id": "cell-3b166fac02ec4711",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In-lab exercise: Laplace smoothing\n",
    "\n",
    "Run the code below and figure out why it fails.\n",
    "\n",
    "When a word does not appear with a specific class in the training data, its class-conditional probability is 0, and we are unable to\n",
    "get a reasonable probability for that class.\n",
    "\n",
    "Research Laplace smoothing, and modify the code above to implement Laplace smoothing (setting the frequency of all words with frequency 0 to a frequency of 1).\n",
    "Run the modified code on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting prediction for \"this is the book\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8095ea998923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Getting prediction for \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-1e60a1384353>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprob_statement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassCondProb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'statement'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'statement'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprob_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassCondProb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'question'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m  \u001b[0mprob_statement\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mprob_question\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1e60a1384353>\u001b[0m in \u001b[0;36mclassCondProb\u001b[0;34m(sentence, className)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclassName\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'statement'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list_s\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprob_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list_q\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 1 with size 0"
     ]
    }
   ],
   "source": [
    "test_docs = list([test['sentence'] for index,test in testing_data.iterrows()])\n",
    "print('Getting prediction for \"%s\"' % test_docs[0])\n",
    "predict(test_docs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c92cc1750ff6519f167950df416dbad7",
     "grade": false,
     "grade_id": "cell-9d86c9d269d1a550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.1 (10 points)\n",
    "\n",
    "Explain Why it failed and explain how to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a8b94cb831bb4a934e82d63cdf77be3",
     "grade": false,
     "grade_id": "cell-d424d31d1e17fd89",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Explanation here! (Double click to explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The failing has occured because we do not have the input word in our dictionary, the dictionary that we made using the training datasets for both of the classes separately. So the function idx = np.where(word_list_s == word)[0][0] is generating out of bound index error, since np.where(word_list_s == word)[0] returned empty list indexing empty array generated that error.\\ So this is due to the absence of word in the training dataset for that class. What I did is just checked if the word is present in the word_list_s for statement and word_list_q for question condition. If the word is not present then I skipped that word by setting it's probability to 1 (Laplace smooting) and multiplied with the previous resulting probability, like this: prob = prob * 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a281f0b0b151358f4dbbcbfb6adbd38",
     "grade": false,
     "grade_id": "cell-e17217b48752c1fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2 (20 points)\n",
    "\n",
    "Modify your code and make it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e51fedc8aefb2614a7936134d6333f67",
     "grade": false,
     "grade_id": "cell-0722e2f212d3751c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def prior(className):    \n",
    "    denominator = len(stmt_docs) + len(question_docs)\n",
    "    \n",
    "    if className == 'statement':\n",
    "        numerator =  len(stmt_docs)\n",
    "    else:\n",
    "        numerator =  len(question_docs)\n",
    "        \n",
    "    return np.divide(numerator,denominator)\n",
    "    \n",
    "# Calculate class conditional probability for a sentence\n",
    "    \n",
    "def classCondProb(sentence, className):\n",
    "    words = get_words(sentence)\n",
    "    prob = 1\n",
    "    for word in words:\n",
    "        if className == 'statement':\n",
    "            if word in word_list_s:\n",
    "                idx = np.where(word_list_s == word)\n",
    "                prob = prob * prob_s[np.array(idx)[0,0]]\n",
    "            else:\n",
    "                prob = prob * 1 #laplace smoothing\n",
    "        else:\n",
    "            if word in word_list_q:\n",
    "                idx = np.where(word_list_q == word)\n",
    "                prob = prob * prob_q[np.array(idx)[0,0]] \n",
    "            else:\n",
    "                prob = prob * 1 #laplace smoothing\n",
    "    \n",
    "    return prob\n",
    "\n",
    "# Predict class of a sentence\n",
    "\n",
    "def predict(sentence):\n",
    "    prob_statement = classCondProb(sentence, 'statement') * prior('statement')\n",
    "    prob_question = classCondProb(sentence, 'question') * prior('question')\n",
    "    if  prob_statement > prob_question:\n",
    "        return 'statement'\n",
    "    else:\n",
    "        return 'question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b12f4c8d6b4163d2f6cfe464b3ba62f3",
     "grade": true,
     "grade_id": "cell-c576e7ed4dd3046a",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting prediction for this is the book\"\n",
      "question\n",
      "Getting prediction for who are the novels characters\"\n",
      "question\n",
      "Getting prediction for is this the author\"\n",
      "question\n",
      "Getting prediction for I like apples\"\n",
      "question\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "# Test function: Do not remove\n",
    "test_docs = list([test['sentence'] for index,test in testing_data.iterrows()])\n",
    "\n",
    "for sentence in test_docs:\n",
    "    print('Getting prediction for %s\"' % sentence)\n",
    "    print(predict(sentence))\n",
    "    \n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63a751b5f418dd7b04bf156032e8435f",
     "grade": false,
     "grade_id": "cell-12db07859804f68d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expect result**:\\\n",
    "Getting prediction for this is the book\"\\\n",
    "question\\\n",
    "Getting prediction for who are the novels characters\"\\\n",
    "question\\\n",
    "Getting prediction for is this the author\"\\\n",
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a3302658ee7212f23942b2d4d93e70c",
     "grade": false,
     "grade_id": "cell-2bc1a154cce1dd7a",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Take home exercise\n",
    "\n",
    "Find a more substantial text classification dataset, clean up the documents, and build your NB classifier. Write a brief report on your in-lab and take home exercises and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets**\n",
    "\n",
    "\n",
    "Dataset source: http://www.cs.cornell.edu/people/pabo/movie-review-data \n",
    "This dataset includes two text files, one with positive movie reviews and the other with negative movie reviews. The total number of reviews on both text files is 5331, with one review per line. I'm going to create a Classifier using the Naive Bayes Classifier to determine whether the given review is positive or negative. Because the number of reviews on both classes is the same, I'm going to divide the training and test sets from both classes equally, 20% (almost 1000 reviews per class) for testing and 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive Reviews 5331\n",
      "Number of negative Reviews 5331\n"
     ]
    }
   ],
   "source": [
    "data_pos = pd.read_fwf('positive.txt')\n",
    "data_neg = pd.read_fwf('negative.txt')\n",
    "\n",
    "pdata = data_pos['Positive_text']\n",
    "ndata = data_neg['Negative_text']\n",
    "\n",
    "print(\"Number of positive Reviews\", pdata.shape[0])\n",
    "print(\"Number of negative Reviews\", ndata.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(32)\n",
    "idx = [i for i in range(pdata.shape[0])]\n",
    "random.shuffle(idx)\n",
    "train_len = int(0.8*len(idx))\n",
    "train_idx = idx[:train_len]\n",
    "test_idx = idx[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrain = pdata.iloc[train_idx]\n",
    "ptest = pdata.iloc[test_idx]\n",
    "ntrain = ndata.iloc[train_idx]\n",
    "ntest = ndata.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4264,) (1067,) (4264,) (1067,)\n"
     ]
    }
   ],
   "source": [
    "print(ptrain.shape, ptest.shape, ntrain.shape, ntest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs a series containing words as index with value as count\n",
    "#input is a series of lines\n",
    "def process_text(data):\n",
    "    dp_split = data.str.split()\n",
    "    dppd = pd.DataFrame(dp_split.to_list())\n",
    "    dppd = dppd.fillna(value='bishal')\n",
    "\n",
    "    sums = dppd[dppd[0].str.isalpha()][0].value_counts()\n",
    "    for i in dppd.columns:\n",
    "        if i!=0:\n",
    "            sums = (dppd[dppd[i].str.isalpha()][i].value_counts()).radd(sums, fill_value=0)\n",
    "            \n",
    "    labels = []\n",
    "    count = 0\n",
    "    for word in sums.index:\n",
    "        if len(word)<3 and word!=\"no\":\n",
    "            labels.append(word)\n",
    "            \n",
    "    labels.append('bishal')    \n",
    "    sums.drop(labels = labels, inplace=True)\n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in positive class bag (10372,) \n",
      "Number of words in negative class bag: (10598,)\n",
      "total number of words mixing both classes bags: (15297,)\n"
     ]
    }
   ],
   "source": [
    "#bag of words for pos and neg class with frequecy\n",
    "pos = process_text(ptrain)\n",
    "neg = process_text(ntrain)\n",
    "\n",
    "print(f\"Number of words in positive class bag {pos.shape} \\nNumber of words in negative class bag: {neg.shape}\")\n",
    "\n",
    "#bag of words in both postive class bag and negative class bag\n",
    "tot = pos.add(neg, fill_value=0, axis = 0)\n",
    "print(\"total number of words mixing both classes bags:\", tot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aaa          1.0\n",
       "aaliyah      3.0\n",
       "aan          1.0\n",
       "abandon      8.0\n",
       "abandone     1.0\n",
       "abandoned    2.0\n",
       "abandono     1.0\n",
       "abbas        1.0\n",
       "abbass       1.0\n",
       "abbott       2.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos/tot[pos.index]\n",
    "tot.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Preprocessing**\n",
    "\n",
    "The dataset is in text format and is well formatted, with each review on its own line. As a result, it will be simpler to divide the dataset into train and test sets. And, because the positive and negative reviews are in separate files, I can load them separately with a simple file loader.\n",
    "\n",
    "Though the dataset is well formatted, the reviews themselves are not so clean, implying that the reviewer has given reviews in their own unique manner. Some reviews are in different languages, with non-english or near-english symbols (possibly a French character, I'm not sure:)). There are also other numeric characters (0-9) and symbols such as the dollar sign, at sign, comma, single quote, double quote, dash, tilde, and a few others. It would be simple to filter this out if they were written separately, but that was not the case. They are written within a text. Another major issue is the use of word with an apostrophe sign in between the characters in word.\n",
    "Using a simple strip method, the symbols on the outside of the word can be removed. However, I was unable to process the text that contained symbols within characters.\n",
    "\n",
    "Another task that we can perform is word stemming, which is the process of replacing word variants with the base word. For example, the word eat can take several forms, including eaten, ate, eats, and eating. This example may not be useful in distinguishing between positive and negative reviews, but I hope it suffices. It would be preferable if we could simply do that.\n",
    "\n",
    "So, what exactly did I do? I simply checked whether the word contained any characters other than the alphabets and filtered out any words that did. This is the most basic form of text processing, as well as the worst, because it may replace many important words (the words that serve the purpose to distinguish between positive and negative reviews). It turned out that there weren't many of these words. The total number of words in each class bag is around 10,000, and the total number of words in the collection of these bags is around 15,000. As a result, I was confident that this would assist me in developing a decently performing model.\n",
    "\n",
    "I removed the word 'the' which has count around 5000, almost same as number of reviews, and some other two letter words (not-so-important word and some of them are not even a word) except some important one from the training datasets.\n",
    "\n",
    "I didn't use any fancy text processing library. I just used the pandas to load the dataset and process the texts and store the bag of words and almost everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there is equal number of positive and negative review in our training dataset the priors will be\n",
    "#equal to 0.5\n",
    "prob_y_pos = ptrain.shape[0]/(ntrain.shape[0]+ptrain.shape[0])\n",
    "prob_y_neg = ntrain.shape[0]/(ntrain.shape[0]+ptrain.shape[0])\n",
    "\n",
    "#probability of each word in its class\n",
    "prob_word_pos = pos/tot[pos.index]\n",
    "prob_word_neg = neg/tot[neg.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains parameters like priors of each class, which in my case it's 0.5, since the number of data for each class is the same. And the other parameters are the probability of texts for each class. The calculations are made easy by the pandas dataframe and series data structures. Like wise during testing, the accessing of the probability of each words from the collection of bag of words is also made easy by the pandas series data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to process the text sentence\n",
    "def process_textsent(dppd, index): #dataframe of text, index to process the text at index\n",
    "    return dppd.iloc[index][dppd.iloc[index].str.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_p_n_prob(sentence, prob_word_pos, prob_word_neg, prob_y_neg, prob_y_pos):\n",
    "    pi_p = 1\n",
    "    pi_n = 1\n",
    "    for word in sentence.values:\n",
    "        if word in prob_word_neg:\n",
    "            pi_n = pi_n * prob_word_neg[word]\n",
    "        if word in prob_word_pos:\n",
    "            pi_p = pi_p * prob_word_pos[word]\n",
    "\n",
    "    prob_s_n = prob_y_neg * pi_n\n",
    "    prob_s_p = prob_y_pos * pi_p\n",
    "    \n",
    "    return prob_s_p, prob_s_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_testdb(df):\n",
    "    dp_split = df.str.split()\n",
    "    dppd = pd.DataFrame(dp_split.to_list())\n",
    "    dppd = dppd.fillna(value='@') #so that I could remove this while processing the text\n",
    "    return dppd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ptest = process_testdb(ptest)\n",
    "processed_ntest = process_testdb(ntest)\n",
    "\n",
    "#testing for positive reviews\n",
    "sum_predict_p = 0\n",
    "for index in range(processed_ptest.shape[0]):\n",
    "    sentence = process_textsent(processed_ptest, index);\n",
    "    prob_p, prob_n = sentence_p_n_prob(sentence, prob_word_pos, prob_word_neg, prob_y_neg, prob_y_pos)\n",
    "    if prob_p >= prob_n:\n",
    "        sum_predict_p += 1\n",
    "\n",
    "#testing for negative reviews\n",
    "sum_predict_n = 0\n",
    "for index in range(processed_ntest.shape[0]):\n",
    "    sentence = process_textsent(processed_ntest, index);\n",
    "    prob_p, prob_n = sentence_p_n_prob(sentence, prob_word_pos, prob_word_neg, prob_y_neg, prob_y_pos)\n",
    "    if prob_n >= prob_p:\n",
    "        sum_predict_n += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for positive class: 803/1067 \n",
      "Score for negative class: 788/1067\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score for positive class: {sum_predict_p}/{ptest.shape[0]} \\nScore for negative class: {sum_predict_n}/{ntest.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct postive reviews:  75.25773195876289\n",
      "Percentage of correct negative reviews:  73.8519212746017\n",
      "Total accuracy of model:  74.55482661668229\n"
     ]
    }
   ],
   "source": [
    "pos_per = sum_predict_p*100/ptest.shape[0]\n",
    "neg_per = sum_predict_n*100/ntest.shape[0]\n",
    "print(\"Percentage of correct postive reviews: \", pos_per)\n",
    "print(\"Percentage of correct negative reviews: \", neg_per)\n",
    "print(\"Total accuracy of model: \", (pos_per + neg_per) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "My model's scores for each class are listed below: \\ Positive class score: 803/1067 score for negative class: 788/1067\n",
    "My model correctly classifies 803 of the 1067 reviews for the positive class and 788 for the negative class.\n",
    "The percentage scores are as follows: 75.25773195876289% of correct positive reviews 73.8519212746017% of correct negative reviews\n",
    "So, based on 2000 reviews (1000 for each class), the overall accuracy of my model is 74.44%.\n",
    "The performance can be improved by properly preprocessing text datasets and using a large number of such training datasets. No words should be filtered out solely based on their character composition. Proper word stemming may also result in improved model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
